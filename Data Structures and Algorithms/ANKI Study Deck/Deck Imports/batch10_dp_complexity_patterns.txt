#separator:Tab
#html:true
#tags column:3
#deck:DSA Master Deck
What is Dynamic Programming (DP) and what two properties make a problem solvable with DP?	<b>Dynamic Programming</b> solves problems by breaking them into smaller subproblems, solving each <b>once</b>, and storing results to avoid recomputation. Turns exponential → polynomial.<br><br><b>Two required properties:</b><br>1. <b>Optimal substructure:</b> The optimal solution contains optimal solutions to subproblems. (e.g., shortest path through B uses shortest path TO B)<br>2. <b>Overlapping subproblems:</b> The same subproblems are solved multiple times. (e.g., fib(3) is needed by both fib(4) and fib(5))<br><br><b>Analogy:</b> DP is like taking notes — instead of re-deriving answers from scratch each time, you write them down and look them up.<br><br><b>If subproblems DON'T overlap</b> → use divide and conquer (merge sort). If no optimal substructure → DP won't help.	dp::fundamentals
What is the difference between top-down (memoization) and bottom-up (tabulation)?	<b>Top-Down (Memoization):</b> Start with the big problem, recurse into subproblems, cache results:<br><pre><code>def fib(n, memo={}):<br>    if n <= 1: return n<br>    if n in memo: return memo[n]<br>    memo[n] = fib(n-1, memo) + fib(n-2, memo)<br>    return memo[n]</code></pre><br><b>Bottom-Up (Tabulation):</b> Start with smallest subproblems, build up to the answer iteratively:<br><pre><code>def fib(n):<br>    if n <= 1: return n<br>    dp = [0] * (n + 1)<br>    dp[1] = 1<br>    for i in range(2, n + 1):<br>        dp[i] = dp[i-1] + dp[i-2]<br>    return dp[n]</code></pre><br><b>Top-down:</b> Easier to write (just add memo to recursion), only solves needed subproblems. <b>Bottom-up:</b> No recursion overhead, easier to optimize space, generally faster in practice.	dp::fundamentals
What is the 4-stage progression for learning DP?	<b>Stage 1: Pure Recursion</b> — Understand the structure and recurrence<br><pre><code>def climb(n):<br>    if n <= 2: return n<br>    return climb(n-1) + climb(n-2)  # O(2ⁿ)</code></pre><br><b>Stage 2: Add Memoization</b> — Cache overlapping subproblems<br><pre><code>def climb(n, memo={}):<br>    if n <= 2: return n<br>    if n not in memo:<br>        memo[n] = climb(n-1, memo) + climb(n-2, memo)<br>    return memo[n]  # O(n)</code></pre><br><b>Stage 3: Convert to DP Table</b> — Bottom-up iteration<br><pre><code>dp = [0]*(n+1); dp[1], dp[2] = 1, 2<br>for i in range(3, n+1): dp[i] = dp[i-1]+dp[i-2]  # O(n)</code></pre><br><b>Stage 4: Space Optimization</b> — Keep only what you need<br><pre><code>prev2, prev1 = 1, 2<br>for i in range(3, n+1):<br>    prev2, prev1 = prev1, prev1+prev2  # O(1) space</code></pre><br>Don't jump straight to DP tables — understand the recursion first!	dp::fundamentals
How do you recognize a DP problem in an interview?	<b>Signal words:</b><br>• "Maximum" / "Minimum" (min cost, max profit)<br>• "Count the number of ways"<br>• "Longest" / "Shortest" (subsequence, path)<br>• "Is it possible" (can you reach, can you partition)<br>• "Optimal" (best strategy, fewest operations)<br><br><b>Problem structure signals:</b><br>• Choices at each step (take or leave, use coin or don't)<br>• Answer depends on previous decisions<br>• Brute force would try all combinations (exponential)<br>• Greedy doesn't work (local optimal ≠ global optimal)<br><br><b>NOT DP if:</b><br>• Greedy works (activity selection with clear sorting criterion)<br>• Need ALL solutions explicitly (use backtracking)<br>• No overlapping subproblems (use divide and conquer)	dp::fundamentals
How do you define the DP state (the "dp[i] means..." step)?	The <b>state definition</b> is the most important step — it determines what dp[i] (or dp[i][j]) represents:<br><br><b>Framework:</b><br>1. <b>What question does dp[i] answer?</b> — "The minimum cost to reach step i" or "The number of ways to make amount i"<br>2. <b>What are the dimensions?</b> — Usually matches the changing variables in the recurrence<br>3. <b>What's the base case?</b> — The smallest/simplest input with a known answer<br><br><b>Common state definitions:</b><br>• <code>dp[i]</code> = answer for the first i elements, or answer for value i<br>• <code>dp[i][j]</code> = answer using first i items with capacity j (knapsack)<br>• <code>dp[i][j]</code> = answer for substring s[i:j] (palindrome, parsing)<br>• <code>dp[i][j]</code> = answer comparing s1[:i] and s2[:j] (LCS, edit distance)<br><br><b>Test your definition:</b> Can you express dp[i] in terms of smaller dp values? If yes, you have the right state.	dp::fundamentals
How do you solve Climbing Stairs (the simplest DP problem)?	<b>dp[i] = number of ways to reach step i</b> (can take 1 or 2 steps):<br><pre><code>def climbStairs(n):<br>    if n <= 2: return n<br>    prev2, prev1 = 1, 2  # dp[1]=1, dp[2]=2<br>    for i in range(3, n + 1):<br>        curr = prev1 + prev2  # from i-1 or i-2<br>        prev2, prev1 = prev1, curr<br>    return prev1</code></pre><br><b>Recurrence:</b> dp[i] = dp[i-1] + dp[i-2]. Reach step i from step i-1 (1 step) or i-2 (2 steps).<br><br><b>Time: O(n), Space: O(1)</b> with space optimization.<br><br>This is the Fibonacci sequence! dp[1]=1, dp[2]=2, dp[3]=3, dp[4]=5. The pattern: current state depends only on the previous two → keep just two variables.	dp::1d
How do you solve Coin Change (minimum coins to make amount)?	<b>dp[amount] = minimum coins needed to make that amount</b>:<br><pre><code>def coinChange(coins, amount):<br>    dp = [float('inf')] * (amount + 1)<br>    dp[0] = 0  # 0 coins for amount 0<br>    for a in range(1, amount + 1):<br>        for coin in coins:<br>            if coin <= a:<br>                dp[a] = min(dp[a], dp[a - coin] + 1)<br>    return dp[amount] if dp[amount] != float('inf') else -1</code></pre><br><b>Recurrence:</b> dp[a] = min(dp[a - coin] + 1) for each valid coin. "What's the fewest coins for amount a-coin, plus this one coin?"<br><br><b>Time: O(amount × len(coins)), Space: O(amount)</b>.<br><br>Initialize with infinity (impossible). dp[0]=0 is the base case. If dp[amount] is still infinity, it's impossible. This is an <b>unbounded knapsack</b> variant — can reuse coins.	dp::1d
How do you solve House Robber (max sum, no two adjacent)?	<b>dp[i] = max money from houses 0..i</b>. At each house: rob it (skip previous) or skip it:<br><pre><code>def rob(nums):<br>    if len(nums) <= 2: return max(nums)<br>    prev2, prev1 = nums[0], max(nums[0], nums[1])<br>    for i in range(2, len(nums)):<br>        curr = max(prev1, prev2 + nums[i])<br>        prev2, prev1 = prev1, curr<br>    return prev1</code></pre><br><b>Recurrence:</b> dp[i] = max(dp[i-1], dp[i-2] + nums[i]).<br>• Skip house i: take dp[i-1]<br>• Rob house i: take dp[i-2] + nums[i] (can't rob i-1)<br><br><b>Time: O(n), Space: O(1)</b>.<br><br><b>House Robber II (circular):</b> Rob houses 0..n-2 OR 1..n-1 (can't rob both first and last). Run House Robber twice, take the max.	dp::1d
How do you solve Longest Increasing Subsequence (LIS)?	<b>dp[i] = length of LIS ending at index i</b>:<br><pre><code>def lengthOfLIS(nums):<br>    n = len(nums)<br>    dp = [1] * n  # each element is a subsequence of length 1<br>    for i in range(1, n):<br>        for j in range(i):<br>            if nums[j] < nums[i]:<br>                dp[i] = max(dp[i], dp[j] + 1)<br>    return max(dp)</code></pre><br><b>Recurrence:</b> dp[i] = max(dp[j] + 1) for all j < i where nums[j] < nums[i].<br><br><b>Time: O(n²), Space: O(n)</b>.<br><br><b>O(n log n) optimization:</b> Maintain a "tails" array where tails[i] = smallest tail element for increasing subsequence of length i+1. Use binary search to find insertion point. This doesn't change the DP concept, just optimizes the inner loop.	dp::1d
How do you solve Word Break (can string be segmented into dictionary words)?	<b>dp[i] = True if s[:i] can be segmented using dictionary words</b>:<br><pre><code>def wordBreak(s, wordDict):<br>    words = set(wordDict)<br>    n = len(s)<br>    dp = [False] * (n + 1)<br>    dp[0] = True  # empty string is valid<br>    for i in range(1, n + 1):<br>        for j in range(i):<br>            if dp[j] and s[j:i] in words:<br>                dp[i] = True<br>                break<br>    return dp[n]</code></pre><br><b>Recurrence:</b> dp[i] = True if any dp[j] is True AND s[j:i] is a dictionary word.<br><br><b>Time: O(n² × k)</b> where k = word length for substring check. <b>Space: O(n)</b>.<br><br><b>Optimization:</b> Instead of checking all j, only check substrings whose length matches a dictionary word. Or iterate backwards from i by max word length.	dp::1d
How do you solve Decode Ways (count decodings of digit string)?	<b>dp[i] = number of ways to decode s[:i]</b>:<br><pre><code>def numDecodings(s):<br>    if not s or s[0] == '0': return 0<br>    n = len(s)<br>    prev2, prev1 = 1, 1  # dp[0]=1 (empty), dp[1]=1<br>    for i in range(2, n + 1):<br>        curr = 0<br>        if s[i-1] != '0':  # single digit valid<br>            curr += prev1<br>        if 10 <= int(s[i-2:i]) <= 26:  # two digits valid<br>            curr += prev2<br>        prev2, prev1 = prev1, curr<br>    return prev1</code></pre><br><b>Recurrence:</b> dp[i] = (dp[i-1] if single digit valid) + (dp[i-2] if two digits valid).<br><br><b>Time: O(n), Space: O(1)</b>.<br><br>Like climbing stairs but with validity constraints. '0' alone is invalid. '06' is invalid. '10'-'26' can be decoded as one letter. Edge case heavy — watch for leading zeros.	dp::1d
How do you solve Longest Common Subsequence (LCS)?	<b>dp[i][j] = LCS length of s1[:i] and s2[:j]</b>:<br><pre><code>def longestCommonSubsequence(text1, text2):<br>    m, n = len(text1), len(text2)<br>    dp = [[0] * (n + 1) for _ in range(m + 1)]<br>    for i in range(1, m + 1):<br>        for j in range(1, n + 1):<br>            if text1[i-1] == text2[j-1]:<br>                dp[i][j] = dp[i-1][j-1] + 1  # match!<br>            else:<br>                dp[i][j] = max(dp[i-1][j], dp[i][j-1])  # skip one<br>    return dp[m][n]</code></pre><br><b>Recurrence:</b><br>• Characters match: dp[i][j] = dp[i-1][j-1] + 1 (extend the LCS)<br>• Don't match: dp[i][j] = max(dp[i-1][j], dp[i][j-1]) (skip from either string)<br><br><b>Time: O(m×n), Space: O(m×n)</b>, reducible to O(n).	dp::2d
How do you solve Edit Distance (min operations to transform s1 → s2)?	<b>dp[i][j] = min edits to convert s1[:i] to s2[:j]</b>:<br><pre><code>def minDistance(word1, word2):<br>    m, n = len(word1), len(word2)<br>    dp = [[0]*(n+1) for _ in range(m+1)]<br>    for i in range(m+1): dp[i][0] = i  # delete all<br>    for j in range(n+1): dp[0][j] = j  # insert all<br>    for i in range(1, m+1):<br>        for j in range(1, n+1):<br>            if word1[i-1] == word2[j-1]:<br>                dp[i][j] = dp[i-1][j-1]  # no edit<br>            else:<br>                dp[i][j] = 1 + min(<br>                    dp[i-1][j],    # delete<br>                    dp[i][j-1],    # insert<br>                    dp[i-1][j-1]  # replace<br>                )<br>    return dp[m][n]</code></pre><br><b>Time: O(m×n), Space: O(m×n)</b>. Three operations: insert, delete, replace — each maps to a different cell in the DP table. Base cases: converting empty string to s2 requires j inserts; converting s1 to empty requires i deletes.	dp::2d
How do you solve 0/1 Knapsack?	<b>dp[i][w] = max value using first i items with capacity w</b>:<br><pre><code>def knapsack(weights, values, capacity):<br>    n = len(weights)<br>    dp = [[0]*(capacity+1) for _ in range(n+1)]<br>    for i in range(1, n+1):<br>        for w in range(1, capacity+1):<br>            dp[i][w] = dp[i-1][w]  # skip item i<br>            if weights[i-1] <= w:<br>                dp[i][w] = max(dp[i][w],<br>                    values[i-1] + dp[i-1][w-weights[i-1]])  # take item i<br>    return dp[n][capacity]</code></pre><br><b>Recurrence:</b> dp[i][w] = max(skip, take) where take = values[i-1] + dp[i-1][w-weights[i-1]].<br><br><b>Time: O(n × capacity), Space: O(n × capacity)</b>, reducible to O(capacity).<br><br><b>0/1</b> means each item used at most once (use dp[i-1]). <b>Unbounded</b> means reuse allowed (use dp[i]). Coin Change is unbounded knapsack.	dp::2d
How do you solve Unique Paths (count paths in grid)?	<b>dp[r][c] = number of paths from (0,0) to (r,c)</b>, moving only right or down:<br><pre><code>def uniquePaths(m, n):<br>    dp = [1] * n  # first row all 1s<br>    for r in range(1, m):<br>        for c in range(1, n):<br>            dp[c] += dp[c-1]  # dp[c] = from above + from left<br>    return dp[n-1]</code></pre><br><b>Recurrence:</b> dp[r][c] = dp[r-1][c] + dp[r][c-1]. Can only arrive from above or from the left.<br><br><b>Time: O(m×n), Space: O(n)</b> with rolling array.<br><br><b>With obstacles:</b> If cell is obstacle, dp[r][c] = 0.<br><b>Min path sum:</b> dp[r][c] = grid[r][c] + min(dp[r-1][c], dp[r][c-1]).	dp::2d
How do you solve Maximal Square (largest square of 1s in a matrix)?	<b>dp[r][c] = side length of largest square with bottom-right corner at (r,c)</b>:<br><pre><code>def maximalSquare(matrix):<br>    if not matrix: return 0<br>    rows, cols = len(matrix), len(matrix[0])<br>    dp = [[0]*cols for _ in range(rows)]<br>    max_side = 0<br>    for r in range(rows):<br>        for c in range(cols):<br>            if matrix[r][c] == '1':<br>                if r == 0 or c == 0:<br>                    dp[r][c] = 1<br>                else:<br>                    dp[r][c] = min(dp[r-1][c], dp[r][c-1], dp[r-1][c-1]) + 1<br>                max_side = max(max_side, dp[r][c])<br>    return max_side ** 2</code></pre><br><b>Recurrence:</b> dp[r][c] = min(top, left, top-left) + 1. The square is limited by its weakest neighbor — if any adjacent corner has a smaller square, the current square can only extend by 1 beyond that minimum.<br><br><b>Time: O(m×n), Space: O(m×n)</b>, reducible to O(n).	dp::2d
How do you solve Longest Palindromic Substring?	<b>dp[i][j] = True if s[i:j+1] is a palindrome</b>:<br><pre><code>def longestPalindrome(s):<br>    n = len(s)<br>    if n <= 1: return s<br>    start, max_len = 0, 1<br>    dp = [[False]*n for _ in range(n)]<br>    for i in range(n): dp[i][i] = True  # single chars<br>    for length in range(2, n + 1):  # check all lengths<br>        for i in range(n - length + 1):<br>            j = i + length - 1<br>            if s[i] == s[j]:<br>                dp[i][j] = (length == 2) or dp[i+1][j-1]<br>            if dp[i][j] and length > max_len:<br>                start, max_len = i, length<br>    return s[start:start + max_len]</code></pre><br><b>Time: O(n²), Space: O(n²)</b>.<br><br><b>Expand-from-center approach</b> is simpler and O(1) space: for each center (n single + n-1 between), expand outward while palindrome. Same O(n²) time but more intuitive.	dp::2d
How do you solve Partition Equal Subset Sum?	Determine if array can be split into two subsets with equal sum. This is a <b>0/1 knapsack</b> where target = total/2:<br><pre><code>def canPartition(nums):<br>    total = sum(nums)<br>    if total % 2: return False<br>    target = total // 2<br>    dp = [False] * (target + 1)<br>    dp[0] = True<br>    for num in nums:<br>        for j in range(target, num - 1, -1):  # reverse!<br>            dp[j] = dp[j] or dp[j - num]<br>    return dp[target]</code></pre><br><b>Recurrence:</b> dp[j] = dp[j] or dp[j - num]. "Can we make sum j? Yes if we could already, or if we could make j-num and add this num."<br><br><b>Time: O(n × sum), Space: O(sum)</b>.<br><br><b>Reverse iteration</b> is critical — ensures each number is used at most once (0/1 knapsack). Forward iteration would allow reuse (unbounded).	dp::1d
What is the space optimization trick for 2D DP?	When dp[i] only depends on dp[i-1] (previous row), you only need two rows or even one row:<br><pre><code># Full 2D: O(m×n) space<br>dp = [[0]*(n+1) for _ in range(m+1)]<br><br># Two rows: O(n) space<br>prev = [0] * (n+1)<br>curr = [0] * (n+1)<br>for i in range(1, m+1):<br>    for j in range(1, n+1):<br>        curr[j] = ... using prev[j], prev[j-1], curr[j-1]<br>    prev, curr = curr, [0]*(n+1)<br><br># Single row: O(n) space (when possible)<br>dp = [0] * (n+1)<br>for i in range(1, m+1):<br>    for j in range(1, n+1):<br>        dp[j] = ...  # dp[j] is "above", dp[j-1] is "left"</code></pre><br><b>Key insight:</b> In a single-row approach, <code>dp[j]</code> before update = value from previous row (above). <code>dp[j-1]</code> after update = value from current row (left). This works for unique paths, min path sum, LCS.	dp::fundamentals
What is the complete complexity hierarchy from fastest to slowest?	<pre><code>FASTER ←——————————————————————————————→ SLOWER<br><br>O(1)   O(log n)   O(n)   O(n log n)   O(n²)   O(2ⁿ)   O(n!)<br> ↑        ↑        ↑        ↑          ↑       ↑       ↑<br>Const  Logarithm  Linear  Linearithm  Quad   Expon  Factorial</code></pre><br><b>For n = 1,000,000:</b><br>• O(1): 1 operation<br>• O(log n): ~20 operations<br>• O(n): 1,000,000 operations<br>• O(n log n): ~20,000,000 operations<br>• O(n²): 1,000,000,000,000 — too slow<br>• O(2ⁿ): universe ends first<br><br><b>Rule of thumb:</b> ~10⁸ operations per second. So n=10⁶ is fine for O(n log n), but O(n²) with n=10⁵ is borderline.	complexity::hierarchy
What does each complexity class look like in code?	<b>O(1):</b> Direct access, arithmetic<br><pre><code>return arr[0]  # or dict[key]</code></pre><br><b>O(log n):</b> Halving each step<br><pre><code>while left <= right: mid = (left+right)//2  # binary search</code></pre><br><b>O(n):</b> Single pass<br><pre><code>for x in arr: total += x</code></pre><br><b>O(n log n):</b> Sort then scan, or divide and conquer<br><pre><code>arr.sort()  # Timsort</code></pre><br><b>O(n²):</b> Nested loops over same data<br><pre><code>for i in range(n):<br>    for j in range(n): ...</code></pre><br><b>O(2ⁿ):</b> All subsets, recursive without memoization<br><pre><code>def solve(i): return solve(i-1) + solve(i-2)</code></pre><br><b>O(n!):</b> All permutations<br><pre><code>def permute(remaining): for each: permute(rest)</code></pre>	complexity::hierarchy
How do you calculate space complexity (stack vs heap)?	<b>Space = Stack + Heap</b><br><br><b>Stack:</b> Function call frames from recursion<br><pre><code>def dfs(node):        # each call = one frame<br>    dfs(node.left)    # max depth = O(h) for trees<br>    dfs(node.right)   # O(n) worst for skewed tree</code></pre><br><b>Heap:</b> Data structures you explicitly create<br><pre><code>visited = set()       # O(n) on heap<br>queue = deque()       # O(n) on heap<br>dp = [0] * n          # O(n) on heap</code></pre><br><b>What does NOT count:</b> The input itself, constant-size variables (int, bool), output (since all algorithms produce the same output).<br><br><b>Example — backtracking with BFS queue:</b><br>Stack: O(depth) for recursive calls<br>Heap: O(n) for the queue<br>Total: O(n + depth) = O(n)	complexity::analysis
What are the rules for simplifying Big-O expressions?	<b>Drop constants:</b> O(2n) = O(n), O(n/2) = O(n)<br><br><b>Drop lower-order terms:</b> O(n² + n) = O(n²), O(n + log n) = O(n)<br><br><b>Different variables stay:</b> O(n + m) stays O(n + m) — don't simplify unless you know the relationship<br><br><b>Multiplication stays:</b> O(n × m) stays O(n × m)<br><br><b>Bounded variables simplify:</b> O(n + k) where k ≤ n → O(n)<br><br><b>Sequential = add:</b> Two loops in sequence → O(n) + O(m) = O(n + m)<br><br><b>Nested = multiply:</b> Loop inside loop → O(n) × O(m) = O(n × m)<br><br><b>Show your work first:</b> Say "O(n + k)" before simplifying to "O(n)" — shows the interviewer you understand what drives complexity.	complexity::analysis
What is amortized analysis?	<b>Amortized analysis</b> averages cost over a sequence of operations, giving a more accurate picture than worst-case per operation:<br><br><b>Dynamic array (Python list) append:</b><br>• Most appends: O(1)<br>• Occasional resize: O(n) — copy everything to a bigger array<br>• Amortized over n appends: <b>O(1) per append</b><br><br><b>Why?</b> Resize happens at sizes 1, 2, 4, 8, ... n. Total copy work: 1+2+4+...+n ≈ 2n. Spread over n appends: 2n/n = O(1) each.<br><br><b>Other amortized O(1) operations:</b><br>• Hash map insert (rare rehashing)<br>• Union-Find with path compression: O(α(n)) per operation<br>• Stack-based algorithms where each element is pushed/popped at most once	complexity::analysis
What are the data structure operation complexities you must know?	<b>Array / List:</b> Access O(1), Search O(n), Insert/Delete O(n), Append amortized O(1)<br><br><b>Linked List:</b> Access O(n), Search O(n), Insert/Delete at known position O(1)<br><br><b>Hash Map/Set:</b> Search/Insert/Delete average O(1), worst O(n) — collision<br><br><b>Balanced BST:</b> Search/Insert/Delete O(log n) guaranteed<br><br><b>Heap:</b> Insert O(log n), Extract-min/max O(log n), Peek O(1), Build O(n)<br><br><b>Stack/Queue:</b> Push/Pop/Enqueue/Dequeue O(1)<br><br><b>Trie:</b> Insert/Search O(k) where k = key length<br><br><b>Graph (adjacency list):</b> Add edge O(1), Find edge O(degree), BFS/DFS O(V+E)	complexity::reference
How do you define variables and communicate complexity in interviews?	<b>Always define your variables explicitly:</b><br>"Let n be the number of nodes, m be the number of edges"<br>"Let k be the window size, n be the array length"<br><br><b>Common conventions:</b><br>• n = primary input size (array length, nodes)<br>• m = secondary dimension (edges, matrix columns)<br>• k = constraint or parameter (window size, top-k, sorted distance)<br>• h = height (tree, recursion depth)<br>• L = string/word length<br><br><b>Interview template:</b><br>"The time complexity is O(n log k) — we process n elements, and each heap operation is O(log k) since the heap has at most k elements. Space is O(k) for the heap plus O(h) for the call stack, where h is the tree height."<br><br>Breaking it down shows deeper understanding than just stating "O(n log k)."	complexity::analysis
What is the sliding window pattern?	Maintain a window over a contiguous subarray, expanding/shrinking to meet constraints:<br><pre><code># Max sum subarray of size k (fixed window)<br>def maxSum(arr, k):<br>    window = sum(arr[:k])<br>    best = window<br>    for i in range(k, len(arr)):<br>        window += arr[i] - arr[i-k]  # slide: add right, remove left<br>        best = max(best, window)<br>    return best<br><br># Longest substring without repeating (variable window)<br>def lengthOfLongest(s):<br>    seen = {}<br>    left = best = 0<br>    for right, char in enumerate(s):<br>        if char in seen and seen[char] >= left:<br>            left = seen[char] + 1<br>        seen[char] = right<br>        best = max(best, right - left + 1)<br>    return best</code></pre><br><b>Fixed window:</b> Size stays constant, slide right.<br><b>Variable window:</b> Expand right pointer, shrink left when constraint violated.<br><b>Time: O(n)</b> — each element enters/exits window at most once.	patterns::sliding_window
What is the two-pointer pattern?	Two pointers move through data structure, typically from opposite ends or at different speeds:<br><pre><code># Two Sum (sorted array) — opposite ends<br>def twoSum(arr, target):<br>    left, right = 0, len(arr) - 1<br>    while left < right:<br>        s = arr[left] + arr[right]<br>        if s == target: return [left, right]<br>        elif s < target: left += 1<br>        else: right -= 1<br><br># Remove duplicates in-place — slow/fast<br>def removeDuplicates(arr):<br>    if not arr: return 0<br>    slow = 0<br>    for fast in range(1, len(arr)):<br>        if arr[fast] != arr[slow]:<br>            slow += 1<br>            arr[slow] = arr[fast]<br>    return slow + 1</code></pre><br><b>Opposite ends:</b> Sorted array problems, palindrome check, container with most water.<br><b>Slow/fast:</b> Cycle detection, remove duplicates, linked list middle.<br><b>Time: O(n)</b> — avoids O(n²) brute force.	patterns::two_pointer
What is the binary search pattern beyond sorted arrays?	Binary search works on any <b>monotonic</b> condition — not just sorted arrays:<br><pre><code># Standard: find target in sorted array<br>def binarySearch(arr, target):<br>    lo, hi = 0, len(arr) - 1<br>    while lo <= hi:<br>        mid = (lo + hi) // 2<br>        if arr[mid] == target: return mid<br>        elif arr[mid] < target: lo = mid + 1<br>        else: hi = mid - 1<br>    return -1<br><br># Search on answer: min capacity to ship in D days<br>def shipWithinDays(weights, days):<br>    lo, hi = max(weights), sum(weights)<br>    while lo < hi:<br>        mid = (lo + hi) // 2<br>        if canShip(weights, days, mid):<br>            hi = mid<br>        else:<br>            lo = mid + 1<br>    return lo</code></pre><br><b>"Binary search on the answer":</b> When the answer space is monotonic (if capacity X works, X+1 also works), binary search on possible answers. Time: O(n log(range)).	patterns::binary_search
What is the greedy algorithm pattern?	Make the <b>locally optimal choice</b> at each step, hoping it leads to a globally optimal solution:<br><pre><code># Activity Selection: max non-overlapping intervals<br>def maxActivities(intervals):<br>    intervals.sort(key=lambda x: x[1])  # sort by end time<br>    count = 1<br>    last_end = intervals[0][1]<br>    for start, end in intervals[1:]:<br>        if start >= last_end:<br>            count += 1<br>            last_end = end<br>    return count</code></pre><br><b>When greedy works:</b><br>• Greedy choice property: local optimum leads to global optimum<br>• Interval scheduling, Huffman coding, fractional knapsack<br><br><b>When greedy DOESN'T work:</b><br>• 0/1 Knapsack (must consider all combinations → DP)<br>• Coin Change with arbitrary denominations<br><br><b>Test:</b> Can you find a counterexample where greedy fails? If yes → use DP.	patterns::greedy
What is the monotonic stack pattern?	A stack that maintains elements in sorted order, used for "next greater/smaller element" problems:<br><pre><code># Next Greater Element for each position<br>def nextGreater(arr):<br>    n = len(arr)<br>    result = [-1] * n<br>    stack = []  # stores indices<br>    for i in range(n):<br>        while stack and arr[i] > arr[stack[-1]]:<br>            result[stack.pop()] = arr[i]<br>        stack.append(i)<br>    return result<br><br># [4, 2, 1, 5, 3] → [5, 5, 5, -1, -1]</code></pre><br><b>Time: O(n)</b> — each element pushed and popped at most once.<br><br><b>Use for:</b> Next greater/smaller element, daily temperatures, largest rectangle in histogram, stock span, trapping rain water.<br><br><b>Key insight:</b> When you encounter a larger element, it's the answer for everything smaller currently on the stack.	patterns::stack
What is the prefix sum pattern?	Precompute cumulative sums to answer range queries in O(1):<br><pre><code># Build prefix sum<br>def prefixSum(arr):<br>    prefix = [0] * (len(arr) + 1)<br>    for i in range(len(arr)):<br>        prefix[i+1] = prefix[i] + arr[i]<br>    return prefix<br><br># Range sum query: sum of arr[left:right+1]<br>def rangeSum(prefix, left, right):<br>    return prefix[right+1] - prefix[left]<br><br># Subarray Sum Equals K (with hash map)<br>def subarraySum(nums, k):<br>    count = 0<br>    curr_sum = 0<br>    sums = {0: 1}  # prefix_sum → count<br>    for num in nums:<br>        curr_sum += num<br>        count += sums.get(curr_sum - k, 0)<br>        sums[curr_sum] = sums.get(curr_sum, 0) + 1<br>    return count</code></pre><br><b>Build: O(n), Query: O(1)</b>. The hash map variant finds subarrays summing to k in O(n) by checking if <code>curr_sum - k</code> was seen before.	patterns::prefix_sum
What is the "frequency counting" pattern with hash maps?	Use a hash map to count occurrences, enabling O(n) solutions to many problems:<br><pre><code># Two Sum (unsorted) — complement lookup<br>def twoSum(nums, target):<br>    seen = {}<br>    for i, num in enumerate(nums):<br>        complement = target - num<br>        if complement in seen:<br>            return [seen[complement], i]<br>        seen[num] = i<br><br># Find duplicates<br>from collections import Counter<br>freq = Counter(arr)<br>duplicates = [k for k, v in freq.items() if v > 1]<br><br># First unique character<br>def firstUniq(s):<br>    freq = Counter(s)<br>    for i, c in enumerate(s):<br>        if freq[c] == 1: return i<br>    return -1</code></pre><br>Hash maps trade O(n) space for O(1) lookup, turning O(n²) brute force into O(n). The most common optimization pattern in interviews.	patterns::hash_map
What is the "fast and slow pointer" (Floyd's cycle detection) pattern?	Two pointers moving at different speeds to detect cycles or find midpoints:<br><pre><code># Detect cycle in linked list<br>def hasCycle(head):<br>    slow = fast = head<br>    while fast and fast.next:<br>        slow = slow.next<br>        fast = fast.next.next<br>        if slow == fast: return True<br>    return False<br><br># Find middle of linked list<br>def findMiddle(head):<br>    slow = fast = head<br>    while fast and fast.next:<br>        slow = slow.next<br>        fast = fast.next.next<br>    return slow  # middle node</code></pre><br><b>Why it works for cycles:</b> If there's a cycle, fast gains 1 node per step on slow. They must eventually meet inside the cycle. If no cycle, fast reaches the end.<br><br><b>Find cycle start:</b> After they meet, reset one pointer to head. Move both at speed 1. They meet at the cycle's entry point (mathematical proof).	patterns::linked_list
What is the interview problem-solving framework?	<b>Step 1 — Understand (2-3 min):</b> Restate the problem. Clarify edge cases, constraints, input/output format. Ask: sorted? duplicates? negative numbers?<br><br><b>Step 2 — Plan (3-5 min):</b> Think aloud. Identify the pattern (DP? Graph? Two pointer?). State the approach and complexity BEFORE coding.<br><br><b>Step 3 — Code (10-15 min):</b> Write clean code. Use meaningful variable names. Talk through your logic as you code.<br><br><b>Step 4 — Test (3-5 min):</b> Trace through a small example. Check edge cases (empty input, single element, duplicates).<br><br><b>Step 5 — Analyze:</b> State time and space complexity. Discuss trade-offs and potential optimizations.<br><br><b>Pattern recognition shortcut:</b><br>• "Shortest path" → BFS/Dijkstra<br>• "All combinations/permutations" → Backtracking<br>• "Maximum/minimum with choices" → DP<br>• "Sorted input" → Binary search / Two pointer<br>• "Stream/window" → Sliding window<br>• "Connected" → Union-Find / DFS	patterns::interview_meta
What is the master pattern-matching guide for DSA problems?	<b>By data structure signal:</b><br>• Array + sorted → Two pointer, Binary search<br>• Array + subarray → Sliding window, Prefix sum<br>• Array + pairs/lookup → Hash map<br>• String + prefix → Trie<br>• Tree → DFS/BFS, Recursion<br>• Graph → BFS/DFS, Topological sort, Union-Find<br>• Matrix → "Graph in disguise" → BFS/DFS/DP<br>• Intervals → Sort + sweep<br><br><b>By question type:</b><br>• "Count ways" → DP<br>• "Find all" → Backtracking<br>• "Shortest" → BFS (unweighted), Dijkstra (weighted)<br>• "Kth largest/smallest" → Heap or Quickselect<br>• "Top K" → Heap<br>• "Merge K sorted" → Min-heap<br>• "Next greater" → Monotonic stack<br>• "Parentheses valid" → Stack<br>• "Connected components" → Union-Find or DFS<br>• "Schedule/dependency" → Topological sort	patterns::interview_meta
How do you handle "I'm stuck" moments in interviews?	<b>1. Brute force first:</b> "Let me start with the simplest approach, even if it's not optimal." O(n²) is better than silence. Build from there.<br><br><b>2. Work through examples:</b> Trace a small case by hand. Patterns often emerge from concrete examples.<br><br><b>3. Consider related problems:</b> "This reminds me of [known problem]. Could I adapt that approach?"<br><br><b>4. Think about data structures:</b> "What if I used a hash map here? A heap? A stack?" Often the right data structure unlocks the solution.<br><br><b>5. State the bottleneck:</b> "The nested loop is O(n²). How can I avoid re-scanning?" This shows analytical thinking even while stuck.<br><br><b>6. Simplify the problem:</b> Solve a simpler version first (1D before 2D, sorted before unsorted), then generalize.<br><br><b>Interviewers value:</b> Thought process > perfect solution. Communicate continuously, even when struggling.	patterns::interview_meta
