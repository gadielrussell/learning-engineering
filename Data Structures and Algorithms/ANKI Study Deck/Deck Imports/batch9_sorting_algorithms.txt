#separator:Tab
#html:true
#tags column:3
#deck:DSA Master Deck
What is Bubble Sort and how does it work?	<b>Bubble Sort</b> repeatedly steps through the list, comparing adjacent elements and swapping them if they're in the wrong order. The largest unsorted element "bubbles up" to its correct position each pass.<br><pre><code>def bubble_sort(arr):<br>    n = len(arr)<br>    for i in range(n):<br>        swapped = False<br>        for j in range(n - i - 1):<br>            if arr[j] > arr[j + 1]:<br>                arr[j], arr[j + 1] = arr[j + 1], arr[j]<br>                swapped = True<br>        if not swapped: break  # optimization: already sorted<br>    return arr</code></pre><br><b>Trace:</b> [5,3,8,1,2] → Pass 1: [3,5,1,2,<b>8</b>] → Pass 2: [3,1,2,<b>5</b>,8] → Pass 3: [1,2,<b>3</b>,5,8] → Pass 4: no swaps, done!<br><br>Best: <b>O(n)</b> with swapped flag. Average/Worst: <b>O(n²)</b>. Space: <b>O(1)</b>. <b>Stable ✓, In-place ✓, Adaptive ✓</b>	sorting::quadratic
What is Selection Sort and how does it work?	<b>Selection Sort</b> finds the minimum element from the unsorted portion and swaps it into the next sorted position. Simple but <b>always O(n²)</b> — no early termination possible.<br><pre><code>def selection_sort(arr):<br>    n = len(arr)<br>    for i in range(n):<br>        min_idx = i<br>        for j in range(i + 1, n):<br>            if arr[j] < arr[min_idx]:<br>                min_idx = j<br>        arr[i], arr[min_idx] = arr[min_idx], arr[i]<br>    return arr</code></pre><br><b>Trace:</b> [5,3,8,1,2] → find min=1, swap → [<b>1</b>,3,8,5,2] → find min=2, swap → [1,<b>2</b>,8,5,3] → ...<br><br>Best/Average/Worst: <b>O(n²)</b>. Space: <b>O(1)</b>. <b>Not stable ✗</b> (swaps can disrupt order). Minimizes <b>swaps</b> (exactly n-1), useful when writes are expensive.	sorting::quadratic
What is Insertion Sort and why should you memorize it? ⭐	<b>Insertion Sort</b> builds the sorted array one element at a time, inserting each new element into its correct position — like sorting playing cards in your hand.<br><pre><code>def insertion_sort(arr):<br>    for i in range(1, len(arr)):<br>        key = arr[i]<br>        j = i - 1<br>        while j >= 0 and arr[j] > key:<br>            arr[j + 1] = arr[j]  # shift right<br>            j -= 1<br>        arr[j + 1] = key  # insert<br>    return arr</code></pre><br><b>Trace:</b> [5,3,8,1,2] → key=3: shift 5 right → [<b>3,5</b>,8,1,2] → key=8: already in place → key=1: shift 8,5,3 right → [<b>1</b>,3,5,8,2] → key=2: shift 8,5,3 right → [1,<b>2</b>,3,5,8]<br><br>Best: <b>O(n)</b> already sorted. Average/Worst: <b>O(n²)</b>. Space: <b>O(1)</b>. <b>Stable ✓, In-place ✓, Adaptive ✓</b>. Andrew recommends memorizing this — most intuitive, great for nearly sorted data.	sorting::quadratic
What makes Insertion Sort special for nearly sorted arrays?	Insertion Sort is <b>adaptive</b>: its runtime depends on how "disordered" the input is. For each element, the inner while loop only runs as far as the element needs to shift.<br><br>• <b>Already sorted:</b> Inner loop never executes → <b>O(n)</b><br>• <b>K-sorted</b> (each element at most k away): Inner loop runs at most k times → <b>O(nk)</b><br>• <b>Reverse sorted:</b> Every element shifts to position 0 → <b>O(n²)</b><br><br>This is why insertion sort is the foundation for understanding the K-Sorted Array problem — the constraint "at most k positions away" directly limits the inner loop iterations.<br><br>Bubble Sort is also adaptive (O(n) when sorted), but Insertion Sort does fewer total operations on average because it doesn't repeatedly scan the entire array.	sorting::quadratic
What is Heap Sort and why should you memorize it? ⭐	<b>Heap Sort</b> transforms the array into a max-heap, then repeatedly extracts the maximum and places it at the end. <b>Guaranteed O(n log n)</b> with O(1) space.<br><pre><code>def heap_sort(arr):<br>    n = len(arr)<br>    def heapify(size, root):<br>        largest = root<br>        left, right = 2*root+1, 2*root+2<br>        if left < size and arr[left] > arr[largest]: largest = left<br>        if right < size and arr[right] > arr[largest]: largest = right<br>        if largest != root:<br>            arr[root], arr[largest] = arr[largest], arr[root]<br>            heapify(size, largest)<br>    # Phase 1: Build max-heap O(n)<br>    for i in range(n//2 - 1, -1, -1):<br>        heapify(n, i)<br>    # Phase 2: Extract max repeatedly O(n log n)<br>    for i in range(n-1, 0, -1):<br>        arr[0], arr[i] = arr[i], arr[0]<br>        heapify(i, 0)<br>    return arr</code></pre><br>Best/Average/Worst: <b>O(n log n)</b>. Space: <b>O(1)</b>. <b>Not stable ✗, In-place ✓</b>. Andrew's #1 recommendation — predictable, efficient, and understanding heaps unlocks many other problems.	sorting::nlogn
Why is building a heap O(n) and not O(n log n)?	Intuitively it seems like n insertions × O(log n) each = O(n log n). But building bottom-up with heapify is <b>O(n)</b> because:<br><br><b>Most nodes are near the bottom</b> and need minimal work:<br>• n/2 leaf nodes: 0 swaps<br>• n/4 nodes at height 1: at most 1 swap<br>• n/8 nodes at height 2: at most 2 swaps<br>• ... 1 root node: at most log n swaps<br><br><b>Total work:</b> Σ (n/2^(h+1)) × h for h = 0 to log n<br>= n × Σ h/2^(h+1) = n × 2 = <b>O(n)</b><br><br>The geometric series converges because the vast majority of nodes are leaves that need zero work. This is why <code>heapify</code> starts from the last non-leaf (<code>n//2 - 1</code>) and works upward, not from the root downward.	sorting::nlogn
What is Merge Sort and how does it work?	<b>Merge Sort</b> divides the array in half recursively until single elements, then merges sorted halves back together:<br><pre><code>def merge_sort(arr):<br>    if len(arr) <= 1: return arr<br>    mid = len(arr) // 2<br>    left = merge_sort(arr[:mid])<br>    right = merge_sort(arr[mid:])<br>    return merge(left, right)<br><br>def merge(left, right):<br>    result = []<br>    i = j = 0<br>    while i < len(left) and j < len(right):<br>        if left[i] <= right[j]:<br>            result.append(left[i]); i += 1<br>        else:<br>            result.append(right[j]); j += 1<br>    result.extend(left[i:])<br>    result.extend(right[j:])<br>    return result</code></pre><br>Best/Average/Worst: <b>O(n log n)</b>. Space: <b>O(n)</b>. <b>Stable ✓</b>, Not in-place ✗.<br><br>log n levels of splitting × O(n) merge work per level = O(n log n). The <code><=</code> in the merge comparison maintains stability.	sorting::nlogn
What is Quick Sort and how does partitioning work?	<b>Quick Sort</b> picks a pivot, partitions elements into smaller/larger groups, then recurses on each side:<br><pre><code>def quick_sort(arr, low=0, high=None):<br>    if high is None: high = len(arr) - 1<br>    if low < high:<br>        pivot_idx = partition(arr, low, high)<br>        quick_sort(arr, low, pivot_idx - 1)<br>        quick_sort(arr, pivot_idx + 1, high)<br>    return arr<br><br>def partition(arr, low, high):<br>    pivot = arr[high]<br>    i = low - 1<br>    for j in range(low, high):<br>        if arr[j] <= pivot:<br>            i += 1<br>            arr[i], arr[j] = arr[j], arr[i]<br>    arr[i+1], arr[high] = arr[high], arr[i+1]<br>    return i + 1</code></pre><br>Best/Average: <b>O(n log n)</b>. Worst: <b>O(n²)</b> — when pivot is always min/max (already sorted input). Space: <b>O(log n)</b> stack. <b>Not stable ✗, In-place ✓</b>.<br><br><b>Pivot matters:</b> Randomized pivot avoids worst-case. Median-of-three is another strategy. Cache-friendly in practice.	sorting::nlogn
What is the complete sorting algorithm comparison matrix?	<table><tr><th>Algorithm</th><th>Best</th><th>Avg</th><th>Worst</th><th>Space</th><th>Stable</th><th>Adaptive</th></tr><tr><td>Bubble</td><td>O(n)</td><td>O(n²)</td><td>O(n²)</td><td>O(1)</td><td>✓</td><td>✓</td></tr><tr><td>Selection</td><td>O(n²)</td><td>O(n²)</td><td>O(n²)</td><td>O(1)</td><td>✗</td><td>✗</td></tr><tr><td>Insertion</td><td>O(n)</td><td>O(n²)</td><td>O(n²)</td><td>O(1)</td><td>✓</td><td>✓</td></tr><tr><td>Heap</td><td>O(n log n)</td><td>O(n log n)</td><td>O(n log n)</td><td>O(1)</td><td>✗</td><td>✗</td></tr><tr><td>Merge</td><td>O(n log n)</td><td>O(n log n)</td><td>O(n log n)</td><td>O(n)</td><td>✓</td><td>✗</td></tr><tr><td>Quick</td><td>O(n log n)</td><td>O(n log n)</td><td>O(n²)</td><td>O(log n)</td><td>✗</td><td>✗</td></tr></table><br><b>Stable:</b> Equal elements keep original relative order.<br><b>Adaptive:</b> Runs faster on partially sorted input.<br><b>In-place:</b> O(1) auxiliary space (excluding recursion stack).<br><br>Need stable? → Merge or Insertion. Need guaranteed? → Heap or Merge. Need in-place + fast? → Heap or Quick.	sorting::comparison
What does "stable" mean in sorting and when does it matter?	A <b>stable</b> sort preserves the relative order of elements with equal keys:<br><pre><code># Students sorted by name, now sort by grade:<br>[("Alice", B), ("Bob", A), ("Carol", B), ("Dave", A)]<br><br># Stable sort by grade:<br>[("Bob", A), ("Dave", A), ("Alice", B), ("Carol", B)]<br># Bob before Dave ✓, Alice before Carol ✓ (original order preserved)<br><br># Unstable sort by grade:<br>[("Dave", A), ("Bob", A), ("Carol", B), ("Alice", B)]<br># Order within same grade is unpredictable</code></pre><br><b>Stable sorts:</b> Merge Sort, Insertion Sort, Bubble Sort, Python's Timsort<br><b>Unstable sorts:</b> Quick Sort, Heap Sort, Selection Sort<br><br><b>When it matters:</b> Multi-key sorting (sort by name, then by grade — need grade-sort to preserve name-sort within same grade), database records, any time equal-key ordering is meaningful.	sorting::comparison
What determines the lower bound of comparison-based sorting?	<b>O(n log n)</b> is the theoretical minimum for comparison-based sorting. No comparison-based algorithm can do better in the worst case.<br><br><b>Proof (decision tree argument):</b><br>• n! possible permutations of the input<br>• Each comparison eliminates at most half the remaining possibilities<br>• Need at least log₂(n!) comparisons to distinguish all permutations<br>• log₂(n!) ≈ n log n (Stirling's approximation)<br><br>This means Quick Sort, Merge Sort, and Heap Sort are all <b>optimal</b> for comparison-based sorting.<br><br><b>Non-comparison sorts</b> (Counting, Radix, Bucket) can beat O(n log n) because they exploit properties of the data (integer keys, limited range) rather than comparing elements.	sorting::theory
What are non-comparison sorts and when can they beat O(n log n)?	<b>Counting Sort — O(n + k)</b> where k = range of values:<br><pre><code>def counting_sort(arr):<br>    if not arr: return arr<br>    min_val, max_val = min(arr), max(arr)<br>    count = [0] * (max_val - min_val + 1)<br>    for x in arr: count[x - min_val] += 1<br>    result = []<br>    for i, c in enumerate(count):<br>        result.extend([i + min_val] * c)<br>    return result</code></pre><br><b>Radix Sort — O(d × (n + k))</b> where d = digits, k = base:<br>Sort digit by digit using a stable sort (counting sort) as subroutine.<br><br><b>Bucket Sort — O(n + k)</b> average when data is uniformly distributed.<br><br><b>Use when:</b> Values are integers in a known range, data has specific distribution patterns, or you need true O(n). <b>Limitation:</b> Extra space required, and only work for specific data types.	sorting::non_comparison
How does Counting Sort work step by step?	<pre><code>def counting_sort(arr):<br>    max_val = max(arr)<br>    count = [0] * (max_val + 1)<br>    # Count occurrences<br>    for x in arr:<br>        count[x] += 1<br>    # Rebuild sorted array<br>    result = []<br>    for val, freq in enumerate(count):<br>        result.extend([val] * freq)<br>    return result</code></pre><br><b>Trace:</b> [4, 2, 2, 8, 3, 3, 1]<br>1. Count: [0, 1, 2, 2, 1, 0, 0, 0, 1] (index = value, count = frequency)<br>2. Rebuild: [1, 2, 2, 3, 3, 4, 8]<br><br><b>Time: O(n + k)</b> where k = max value. <b>Space: O(k)</b>.<br><b>Stable</b> (with the prefix-sum variant).<br><br><b>Works when:</b> Values are non-negative integers with a small range. Impractical when k >> n (e.g., sorting 10 numbers in range 0–1,000,000 wastes space).	sorting::non_comparison
How does Radix Sort work?	Sort digit by digit (least significant to most significant), using a <b>stable</b> sort (counting sort) at each digit position:<br><pre><code>def radix_sort(arr):<br>    if not arr: return arr<br>    max_val = max(arr)<br>    exp = 1<br>    while max_val // exp > 0:<br>        counting_sort_by_digit(arr, exp)<br>        exp *= 10<br>    return arr<br><br>def counting_sort_by_digit(arr, exp):<br>    n = len(arr)<br>    output = [0] * n<br>    count = [0] * 10<br>    for x in arr: count[(x // exp) % 10] += 1<br>    for i in range(1, 10): count[i] += count[i-1]<br>    for i in range(n-1, -1, -1):<br>        digit = (arr[i] // exp) % 10<br>        output[count[digit] - 1] = arr[i]<br>        count[digit] -= 1<br>    arr[:] = output</code></pre><br><b>Time: O(d × (n + k))</b> where d = digits, k = base (10). <b>Space: O(n + k)</b>.<br><br><b>Must use stable sub-sort</b> — otherwise earlier digit orderings get destroyed. LSD (least significant digit first) is most common.	sorting::non_comparison
How does Bucket Sort work?	Distribute elements into buckets based on value range, sort each bucket, then concatenate:<br><pre><code>def bucket_sort(arr):<br>    if not arr: return arr<br>    n = len(arr)<br>    min_val, max_val = min(arr), max(arr)<br>    bucket_range = (max_val - min_val) / n + 1<br>    buckets = [[] for _ in range(n)]<br>    for x in arr:<br>        idx = int((x - min_val) / bucket_range)<br>        buckets[idx].append(x)<br>    result = []<br>    for bucket in buckets:<br>        bucket.sort()  # insertion sort for small buckets<br>        result.extend(bucket)<br>    return result</code></pre><br><b>Time:</b> Average <b>O(n + k)</b> with uniform distribution. Worst <b>O(n²)</b> (all elements in one bucket). <b>Space: O(n + k)</b>.<br><br><b>Best for:</b> Uniformly distributed floating-point numbers, values in a known range. Works by reducing the sorting problem to many smaller sorting problems.	sorting::non_comparison
How does Python's built-in sort work (Timsort)?	Python's <code>sorted()</code> and <code>list.sort()</code> use <b>Timsort</b>, a hybrid of merge sort and insertion sort:<br><br><b>How it works:</b><br>1. Find natural "runs" (already sorted subsequences) in the data<br>2. Extend short runs using <b>insertion sort</b> (fast for small/nearly sorted)<br>3. Merge runs using <b>merge sort</b> strategy<br><br><b>Complexity:</b> Best: <b>O(n)</b> (already sorted). Average/Worst: <b>O(n log n)</b>. Space: <b>O(n)</b>. <b>Stable ✓</b>.<br><br><b>Python sorting API:</b><br><pre><code># Returns new sorted list<br>sorted(arr)                  # ascending<br>sorted(arr, reverse=True)    # descending<br>sorted(arr, key=len)         # by custom key<br>sorted(arr, key=lambda x: x[1])  # by second element<br><br># Sorts in-place (returns None)<br>arr.sort()<br>arr.sort(key=lambda x: -x)   # descending trick</code></pre><br>In interviews, always mention Python uses Timsort — shows you understand the language.	sorting::python
How does Python's key parameter work for custom sorting?	The <code>key</code> function transforms each element into a comparison value. Python sorts by the transformed values:<br><pre><code># Sort strings by length<br>sorted(["cat", "a", "elephant"], key=len)  # ["a", "cat", "elephant"]<br><br># Sort tuples by second element<br>sorted([(1,3), (2,1), (3,2)], key=lambda x: x[1])  # [(2,1), (3,2), (1,3)]<br><br># Multi-key: sort by grade (asc), then name (asc)<br>students = [("Bob", 90), ("Alice", 85), ("Carol", 90)]<br>sorted(students, key=lambda x: (x[1], x[0]))<br># [("Alice", 85), ("Bob", 90), ("Carol", 90)]<br><br># Multi-key: grade descending, name ascending<br>sorted(students, key=lambda x: (-x[1], x[0]))<br># [("Bob", 90), ("Carol", 90), ("Alice", 85)]</code></pre><br><b>The negation trick</b> (<code>-x[1]</code>) works for numbers to reverse one key. For strings, use a separate <code>sorted()</code> call (sort by secondary key first, then primary — stable sort preserves the secondary order).	sorting::python
How do you sort a k-sorted (nearly sorted) array optimally?	Each element is at most k positions from its final sorted position. Use a <b>min-heap of size k+1</b>:<br><pre><code>import heapq<br>def sort_k_sorted(arr, k):<br>    heap = arr[:k+1]  # first k+1 elements<br>    heapq.heapify(heap)  # O(k)<br>    result = []<br>    for i in range(k+1, len(arr)):<br>        result.append(heapq.heappop(heap))  # extract min<br>        heapq.heappush(heap, arr[i])       # add next<br>    while heap:<br>        result.append(heapq.heappop(heap))<br>    return result</code></pre><br><b>Time: O(n log k)</b> — n elements × O(log k) per heap operation.<br><b>Space: O(k)</b> for the heap.<br><br><b>Why it works:</b> If an element is at most k away from its final position, the correct next element must be within the next k+1 candidates. A min-heap of size k+1 always contains it. This was the key insight from your session with Andrew.	sorting::interview_patterns
Why is O(n log k) better than O(n log n) for k-sorted arrays?	<b>The relationship:</b> O(n log k) < O(n log n) whenever k < n.<br><br>• When k = 1: O(n log 1) = <b>O(n)</b> — nearly sorted, almost free<br>• When k = 10: O(n log 10) ≈ <b>O(3.3n)</b> — very fast<br>• When k = √n: O(n log √n) = <b>O(n/2 × log n)</b> — half the work<br>• When k = n: O(n log n) — no improvement (fully unsorted)<br><br><b>Comparison of approaches:</b><br>• Generic sort (ignore k): O(n log n) always<br>• Insertion sort (exploit k): O(nk) — good when k is constant, bad when k grows<br>• Heap of size k+1: <b>O(n log k)</b> — optimal for any k<br><br>The heap approach wins because log k grows much slower than k itself. For k=1000, log(1000) ≈ 10, so heap does ~10 comparisons per element vs insertion sort's ~1000.	sorting::interview_patterns
What is Quickselect and how does it find the kth element?	<b>Quickselect</b> uses Quick Sort's partition but only recurses on the side containing the target:<br><pre><code>import random<br>def quickselect(arr, k):<br>    """Find kth smallest element (0-indexed)."""<br>    if len(arr) == 1: return arr[0]<br>    pivot = random.choice(arr)<br>    lows = [x for x in arr if x < pivot]<br>    highs = [x for x in arr if x > pivot]<br>    pivots = [x for x in arr if x == pivot]<br>    if k < len(lows):<br>        return quickselect(lows, k)<br>    elif k < len(lows) + len(pivots):<br>        return pivot<br>    else:<br>        return quickselect(highs, k - len(lows) - len(pivots))</code></pre><br><b>Average: O(n)</b>, Worst: <b>O(n²)</b> (bad pivots). Space: <b>O(n)</b>.<br><br>Unlike Quick Sort which recurses on BOTH sides, Quickselect only recurses on ONE side → n + n/2 + n/4 + ... = 2n = O(n). Used for "kth largest" problems as an alternative to heap-based O(n log k).	sorting::interview_patterns
How do you sort with the Dutch National Flag algorithm (3-way partition)?	Partition array into three groups using three pointers. Classic for "Sort Colors" (0, 1, 2):<br><pre><code>def sortColors(nums):<br>    low, mid, high = 0, 0, len(nums) - 1<br>    while mid <= high:<br>        if nums[mid] == 0:<br>            nums[low], nums[mid] = nums[mid], nums[low]<br>            low += 1<br>            mid += 1<br>        elif nums[mid] == 1:<br>            mid += 1<br>        else:  # nums[mid] == 2<br>            nums[mid], nums[high] = nums[high], nums[mid]<br>            high -= 1<br>            # don't advance mid — swapped element needs checking</code></pre><br><b>Time: O(n), Space: O(1)</b>. Single pass.<br><br><b>Invariants:</b> Everything before <code>low</code> is 0, between <code>low</code> and <code>mid</code> is 1, after <code>high</code> is 2. Note: when swapping with <code>high</code>, don't advance <code>mid</code> because the swapped value hasn't been examined yet.	sorting::interview_patterns
How do you merge two sorted arrays?	Two-pointer technique — compare fronts and take the smaller:<br><pre><code>def merge(arr1, arr2):<br>    result = []<br>    i = j = 0<br>    while i < len(arr1) and j < len(arr2):<br>        if arr1[i] <= arr2[j]:  # <= for stability<br>            result.append(arr1[i]); i += 1<br>        else:<br>            result.append(arr2[j]); j += 1<br>    result.extend(arr1[i:])<br>    result.extend(arr2[j:])<br>    return result</code></pre><br><b>Time: O(n + m), Space: O(n + m)</b>.<br><br>This is the core of merge sort's "conquer" step. The <code><=</code> (not <code><</code>) ensures <b>stability</b>. The <code>extend</code> at the end handles leftover elements from the longer array. This also applies to merging K sorted lists (use a min-heap).	sorting::interview_patterns
How do you merge sorted array in-place ("Merge Sorted Array" LeetCode 88)?	Given nums1 with extra space at the end and nums2, merge by filling from the <b>back</b>:<br><pre><code>def merge(nums1, m, nums2, n):<br>    i, j, k = m - 1, n - 1, m + n - 1<br>    while i >= 0 and j >= 0:<br>        if nums1[i] >= nums2[j]:<br>            nums1[k] = nums1[i]; i -= 1<br>        else:<br>            nums1[k] = nums2[j]; j -= 1<br>        k -= 1<br>    # Copy remaining nums2 elements (nums1 already in place)<br>    while j >= 0:<br>        nums1[k] = nums2[j]<br>        j -= 1; k -= 1</code></pre><br><b>Time: O(n + m), Space: O(1)</b>.<br><br><b>Why fill from the back?</b> Filling from the front would overwrite unprocessed nums1 elements. From the back, we place the largest elements first into the empty spaces, avoiding overwrites. No extra nums1 copy needed.	sorting::interview_patterns
When should you use each sorting algorithm?	<b>Decision guide:</b><br><br>• Small array (n < 50) → <b>Insertion Sort</b> (low overhead, fast for small n)<br>• Nearly sorted data → <b>Insertion Sort</b> (adaptive, O(n) best case)<br>• Need guaranteed O(n log n) + O(1) space → <b>Heap Sort</b><br>• Need stable sort → <b>Merge Sort</b> (or Timsort in practice)<br>• General purpose, fast in practice → <b>Quick Sort</b> (with random pivot)<br>• Known integer range → <b>Counting Sort</b><br>• Large integers, multiple digits → <b>Radix Sort</b><br>• Uniform distribution → <b>Bucket Sort</b><br>• k-sorted array → <b>Min-heap of size k+1</b> → O(n log k)<br>• Find kth element → <b>Quickselect</b> O(n) avg<br>• In Python → Just use <b>sorted()</b> / <b>list.sort()</b> (Timsort) unless the problem specifically asks for a custom sort	sorting::comparison
How does sorting relate to other algorithms and interview patterns?	Sorting is often a <b>preprocessing step</b> that enables other techniques:<br><br><b>Sorting + Two Pointers:</b> Two Sum (sorted array), 3Sum, container with most water<br><br><b>Sorting + Binary Search:</b> Search in sorted result, kth smallest pair distance<br><br><b>Sorting + Greedy:</b> Activity selection, interval scheduling, meeting rooms<br><br><b>Sorting for Deduplication:</b> Group duplicates together (subsets with dups, permutations with dups)<br><br><b>Sorting for Grouping:</b> Group anagrams (sort each word as key), merge intervals (sort by start)<br><br><b>Custom Sort Order:</b> Largest number ("9" > "34" because "934" > "349")<br><br><b>Interview tip:</b> If a problem becomes easier with sorted input, consider sorting first. O(n log n) sort + O(n) scan = O(n log n) total, which is often acceptable.	sorting::interview_patterns
How do you sort characters in a string in Python?	Strings are immutable in Python, so you must convert to a list, sort, and rejoin:<br><pre><code># Sort characters alphabetically<br>sorted_str = ''.join(sorted("hello"))  # "ehllo"<br><br># Sort by custom criterion<br>''.join(sorted("hello", key=str.lower))  # case-insensitive<br><br># Sort by frequency (most common first)<br>from collections import Counter<br>s = "tree"<br>freq = Counter(s)<br>''.join(sorted(s, key=lambda c: (-freq[c], c)))  # "eert"<br><br># Sort string of digits<br>''.join(sorted("3124"))  # "1234"</code></pre><br>Common interview use: sort characters to create anagram keys (<code>sorted("eat") == sorted("tea")</code>), check if two strings are anagrams, or rearrange string characters.	sorting::python
How do you use Python's sort for common interview patterns?	<pre><code># Sort intervals by start time (merge intervals)<br>intervals.sort(key=lambda x: x[0])<br><br># Sort by multiple keys<br>people.sort(key=lambda x: (x[0], -x[1]))  # height asc, k desc<br><br># Sort with custom comparator (Largest Number)<br>from functools import cmp_to_key<br>def compare(a, b):<br>    if a + b > b + a: return -1  # a should come first<br>    return 1<br>nums = sorted(map(str, nums), key=cmp_to_key(compare))<br><br># Partial sort: find k smallest<br>import heapq<br>heapq.nsmallest(k, arr)  # O(n log k)<br><br># Check if sorted<br>all(arr[i] <= arr[i+1] for i in range(len(arr)-1))</code></pre><br><b>Key Python sorting facts:</b><br>• <code>sorted()</code> returns new list, <code>.sort()</code> is in-place (returns None)<br>• Tuples compare element by element (enables multi-key sorting)<br>• <code>cmp_to_key</code> converts old-style comparators to key functions	sorting::python
What is the "Largest Number" sorting problem?	Given integers, arrange them to form the largest possible number. Use custom comparison:<br><pre><code>from functools import cmp_to_key<br>def largestNumber(nums):<br>    def compare(a, b):<br>        if a + b > b + a: return -1<br>        elif a + b < b + a: return 1<br>        return 0<br>    str_nums = sorted(map(str, nums), key=cmp_to_key(compare))<br>    result = ''.join(str_nums)<br>    return '0' if result[0] == '0' else result<br><br># [3, 30, 34, 5, 9] → "9534330"<br># Compare: "9" vs "5" → "95" > "59" → 9 first<br># Compare: "3" vs "30" → "330" > "303" → 3 first</code></pre><br><b>Time: O(n log n × k)</b> where k = avg digit count. The comparison <code>a+b vs b+a</code> determines which ordering produces the larger number. Edge case: all zeros → return "0" not "0000".	sorting::interview_patterns
What is the "Merge Intervals" pattern?	Sort intervals by start time, then merge overlapping ones:<br><pre><code>def merge(intervals):<br>    intervals.sort(key=lambda x: x[0])<br>    merged = [intervals[0]]<br>    for start, end in intervals[1:]:<br>        if start <= merged[-1][1]:  # overlapping<br>            merged[-1][1] = max(merged[-1][1], end)<br>        else:<br>            merged.append([start, end])<br>    return merged<br><br># [[1,3],[2,6],[8,10],[15,18]] → [[1,6],[8,10],[15,18]]</code></pre><br><b>Time: O(n log n)</b> for the sort. <b>Space: O(n)</b>.<br><br>After sorting by start time, overlapping intervals are guaranteed to be adjacent. Compare each interval's start with the previous merged interval's end. Also applies to: insert interval, meeting rooms, interval intersection.	sorting::interview_patterns
How do you count inversions using merge sort?	An <b>inversion</b> is a pair (i, j) where i < j but arr[i] > arr[j]. Count them during the merge step:<br><pre><code>def countInversions(arr):<br>    if len(arr) <= 1: return arr, 0<br>    mid = len(arr) // 2<br>    left, left_inv = countInversions(arr[:mid])<br>    right, right_inv = countInversions(arr[mid:])<br>    merged, split_inv = mergeCount(left, right)<br>    return merged, left_inv + right_inv + split_inv<br><br>def mergeCount(left, right):<br>    result, inversions = [], 0<br>    i = j = 0<br>    while i < len(left) and j < len(right):<br>        if left[i] <= right[j]:<br>            result.append(left[i]); i += 1<br>        else:<br>            result.append(right[j]); j += 1<br>            inversions += len(left) - i  # all remaining left > right[j]<br>    result.extend(left[i:]); result.extend(right[j:])<br>    return result, inversions</code></pre><br><b>Time: O(n log n)</b>. When a right element is chosen before remaining left elements, ALL remaining left elements form inversions with it. Brute force is O(n²).	sorting::interview_patterns
What is the "Meeting Rooms" pattern (interval scheduling)?	<b>Meeting Rooms I:</b> Can a person attend all meetings? Sort by start, check for overlaps:<br><pre><code>def canAttend(intervals):<br>    intervals.sort(key=lambda x: x[0])<br>    for i in range(1, len(intervals)):<br>        if intervals[i][0] < intervals[i-1][1]:<br>            return False  # overlap<br>    return True</code></pre><br><b>Meeting Rooms II:</b> Minimum rooms needed? Track start/end events:<br><pre><code>def minRooms(intervals):<br>    events = []<br>    for start, end in intervals:<br>        events.append((start, 1))   # meeting starts<br>        events.append((end, -1))    # meeting ends<br>    events.sort()<br>    rooms = max_rooms = 0<br>    for time, delta in events:<br>        rooms += delta<br>        max_rooms = max(max_rooms, rooms)<br>    return max_rooms</code></pre><br><b>Time: O(n log n)</b>. Alternative for Meeting Rooms II: use a min-heap of end times (O(n log n)).	sorting::interview_patterns
How does the "Sort an Array" problem relate to interview expectations?	When asked to "implement a sort," interviewers typically want:<br><br><b>1. Know your options:</b> Name 3+ sorts and their trade-offs<br><br><b>2. Pick the right one:</b><br>• General purpose → Quick Sort or Merge Sort<br>• Guaranteed O(n log n) → Merge Sort or Heap Sort<br>• Integer data with small range → Counting Sort<br><br><b>3. Implement correctly:</b> Andrew's recommendation: memorize <b>Heap Sort</b> (most efficient, predictable) and <b>Insertion Sort</b> (most intuitive, good for small/nearly sorted).<br><br><b>4. Analyze trade-offs:</b> "I chose merge sort because we need stability. Quick sort would be faster in practice but has O(n²) worst case. Heap sort guarantees O(n log n) with O(1) space."<br><br>In Python interviews, using <code>sorted()</code> is often acceptable unless the problem specifically asks for a custom sort implementation.	sorting::interview_patterns
How do you sort a linked list?	<b>Merge sort</b> is ideal for linked lists because it doesn't need random access and uses O(1) extra space (no array copying):<br><pre><code>def sortList(head):<br>    if not head or not head.next: return head<br>    # Find middle with slow/fast pointers<br>    slow, fast = head, head.next<br>    while fast and fast.next:<br>        slow = slow.next<br>        fast = fast.next.next<br>    mid = slow.next<br>    slow.next = None  # split<br>    left = sortList(head)<br>    right = sortList(mid)<br>    return mergeLists(left, right)<br><br>def mergeLists(l1, l2):<br>    dummy = ListNode(0)<br>    curr = dummy<br>    while l1 and l2:<br>        if l1.val <= l2.val:<br>            curr.next = l1; l1 = l1.next<br>        else:<br>            curr.next = l2; l2 = l2.next<br>        curr = curr.next<br>    curr.next = l1 or l2<br>    return dummy.next</code></pre><br><b>Time: O(n log n), Space: O(log n)</b> for recursion stack. Unlike arrays, linked list merge is O(1) space — just re-point pointers, no new array needed.	sorting::interview_patterns
What is the "Group Anagrams" sorting pattern?	Sort each word's characters to create a canonical key, then group by key:<br><pre><code>from collections import defaultdict<br>def groupAnagrams(strs):<br>    groups = defaultdict(list)<br>    for s in strs:<br>        key = ''.join(sorted(s))  # "eat" → "aet"<br>        groups[key].append(s)<br>    return list(groups.values())<br><br># Alternative: tuple of character counts as key (avoids sort)<br>def groupAnagrams(strs):<br>    groups = defaultdict(list)<br>    for s in strs:<br>        count = [0] * 26<br>        for c in s: count[ord(c) - ord('a')] += 1<br>        groups[tuple(count)].append(s)<br>    return list(groups.values())</code></pre><br><b>Sort approach:</b> O(n × k log k) where k = max word length.<br><b>Count approach:</b> O(n × k) — faster but less intuitive.<br><br>This demonstrates sorting as a <b>normalization tool</b> — transforming different representations of the same thing into a single canonical form.	sorting::interview_patterns
How do you find the median of two sorted arrays?	Binary search on the shorter array to find the correct partition. This is a hard problem that combines sorting knowledge with binary search:<br><pre><code>def findMedian(nums1, nums2):<br>    if len(nums1) > len(nums2): nums1, nums2 = nums2, nums1<br>    m, n = len(nums1), len(nums2)<br>    lo, hi = 0, m<br>    while lo <= hi:<br>        i = (lo + hi) // 2<br>        j = (m + n + 1) // 2 - i<br>        left1 = nums1[i-1] if i > 0 else float('-inf')<br>        right1 = nums1[i] if i < m else float('inf')<br>        left2 = nums2[j-1] if j > 0 else float('-inf')<br>        right2 = nums2[j] if j < n else float('inf')<br>        if left1 <= right2 and left2 <= right1:<br>            if (m+n) % 2: return max(left1, left2)<br>            return (max(left1,left2) + min(right1,right2)) / 2<br>        elif left1 > right2: hi = i - 1<br>        else: lo = i + 1</code></pre><br><b>Time: O(log min(m,n))</b>. Binary search on the shorter array to find a partition where all left elements ≤ all right elements. Classic hard problem — know the concept even if you can't code it perfectly.	sorting::interview_patterns
