#separator:Tab
#html:true
#tags column:3
#deck:DSA Master Deck
What is a trie (prefix tree) and what problem does it solve?	A <b>trie</b> is a tree-shaped data structure where each path from root to a node represents a <b>prefix</b> of stored strings. Each node has up to <b>alphabet_size</b> children (26 for lowercase English).<br><br>Unlike a hash map which stores whole words, a trie shares prefixes: "cat", "car", "card" share the path c→a, then branch at 't' vs 'r'.<br><pre><code>       root<br>      / \<br>     c   d<br>    /     \<br>   a       o<br>  / \      \<br> t   r      g*<br> *   *<br>     |<br>     d*</code></pre><br><b>Key advantage:</b> Operations depend on <b>word length k</b>, not dictionary size n. Search/insert are O(k) regardless of how many words are stored.	tries::fundamentals
What is the TrieNode structure in Python?	<pre><code>class TrieNode:<br>    def __init__(self):<br>        self.children = {}   # char → TrieNode<br>        self.is_end = False  # marks end of a complete word<br><br>class Trie:<br>    def __init__(self):<br>        self.root = TrieNode()</code></pre><br><b>children</b> can be a dictionary (flexible, any character) or a fixed-size array of 26 (faster for lowercase English):<br><pre><code># Array approach<br>self.children = [None] * 26<br># Access: node.children[ord(char) - ord('a')]</code></pre><br>Dict is more Pythonic and handles any character set. Array is used in performance-critical or interview whiteboard settings.	tries::fundamentals
How do you insert a word into a trie?	Traverse character by character, creating nodes as needed, then mark the last node as a word ending:<br><pre><code>def insert(self, word):<br>    node = self.root<br>    for char in word:<br>        if char not in node.children:<br>            node.children[char] = TrieNode()<br>        node = node.children[char]<br>    node.is_end = True</code></pre><br><b>Time: O(k)</b> where k = word length. <b>Space: O(k)</b> in the worst case (all new nodes). If the prefix already exists, no new nodes are created for shared characters — this is the space-saving power of tries.	tries::operations
How do you search for a word in a trie?	Traverse character by character. The word exists only if you reach the end AND <code>is_end</code> is True:<br><pre><code>def search(self, word):<br>    node = self.root<br>    for char in word:<br>        if char not in node.children:<br>            return False<br>        node = node.children[char]<br>    return node.is_end  # must be a complete word</code></pre><br><b>Time: O(k)</b>. The <code>is_end</code> check is critical: "car" might exist as a prefix path in the trie (because "card" was inserted), but "car" is only a valid word if its final node has <code>is_end = True</code>.	tries::operations
How do you check if any word starts with a given prefix?	Same as search but <b>don't</b> check <code>is_end</code> — any valid path is a prefix:<br><pre><code>def startsWith(self, prefix):<br>    node = self.root<br>    for char in prefix:<br>        if char not in node.children:<br>            return False<br>        node = node.children[char]<br>    return True  # prefix exists regardless of is_end</code></pre><br><b>Time: O(k)</b> where k = prefix length. This is the trie's killer feature — prefix matching is O(k) regardless of dictionary size. A hash map would need to check every stored word for prefix matches.	tries::operations
How do you delete a word from a trie?	Recursively traverse to the word's end, then clean up nodes that are no longer needed:<br><pre><code>def delete(self, word):<br>    def _delete(node, word, depth):<br>        if depth == len(word):<br>            if not node.is_end: return False<br>            node.is_end = False<br>            return len(node.children) == 0  # can delete if no children<br>        char = word[depth]<br>        if char not in node.children: return False<br>        should_delete = _delete(node.children[char], word, depth + 1)<br>        if should_delete:<br>            del node.children[char]<br>            return not node.is_end and len(node.children) == 0<br>        return False<br>    _delete(self.root, word, 0)</code></pre><br><b>Time: O(k)</b>. Only remove nodes that aren't shared with other words. A node is safe to delete if it has no children and isn't the end of another word.	tries::operations
What are the time and space complexities for all trie operations?	<b>All operations are O(k)</b> where k = word/prefix length:<br><br>• Insert: <b>O(k)</b> time, <b>O(k)</b> space (worst case)<br>• Search: <b>O(k)</b> time, <b>O(1)</b> space<br>• Prefix search: <b>O(k)</b> time, <b>O(1)</b> space<br>• Delete: <b>O(k)</b> time, <b>O(1)</b> space<br>• Collect all words with prefix: <b>O(k + total chars in matches)</b><br><br><b>Total space for n words of average length L:</b> O(n × L) worst case, but in practice much less due to shared prefixes. The more words share prefixes, the more space-efficient the trie becomes.	tries::fundamentals
When should you use a trie vs a hash set/hash map?	<b>Use a trie when:</b><br>• <b>Prefix matching</b> is needed (autocomplete, typeahead)<br>• You need to find all words with a given prefix<br>• <b>Alphabetical ordering</b> of results matters (trie gives sorted order for free via DFS)<br>• Building spell checkers or word suggestion systems<br>• The problem involves shared prefixes heavily<br><br><b>Use a hash set/map when:</b><br>• You only need exact word lookup (O(1) average)<br>• No prefix operations needed<br>• Space is very constrained (tries have pointer overhead per node)<br>• Dictionary is small<br><br><b>Trade-off:</b> Trie is O(k) for search (k = word length). Hash map is O(k) average too (must hash the word), but with lower constant factor and no prefix support.	tries::fundamentals
How do you collect all words in a trie that start with a given prefix?	Navigate to the prefix's end node, then DFS to collect all complete words below it:<br><pre><code>def wordsWithPrefix(self, prefix):<br>    node = self.root<br>    for char in prefix:<br>        if char not in node.children:<br>            return []<br>        node = node.children[char]<br>    # DFS to collect all words from this node<br>    result = []<br>    def dfs(node, path):<br>        if node.is_end:<br>            result.append(prefix + ''.join(path))<br>        for char, child in node.children.items():<br>            path.append(char)<br>            dfs(child, path)<br>            path.pop()<br>    dfs(node, [])<br>    return result</code></pre><br><b>Time: O(k + total characters in results)</b>. Navigate O(k) to the prefix node, then DFS touches every descendant. This is the autocomplete pattern — type "hel" and get ["help", "hello", "helmet"].	tries::patterns
How does the "Word Search II" problem use a trie?	Given a board of letters and a word list, find all words that can be formed by adjacent cells. Build a trie from the word list, then DFS the board using the trie to prune:<br><pre><code>def findWords(board, words):<br>    trie = Trie()<br>    for word in words: trie.insert(word)<br>    result = set()<br>    rows, cols = len(board), len(board[0])<br>    def dfs(r, c, node, path):<br>        if node.is_end:<br>            result.add(path)<br>            node.is_end = False  # avoid duplicates<br>        if r<0 or r>=rows or c<0 or c>=cols: return<br>        char = board[r][c]<br>        if char not in node.children: return<br>        board[r][c] = '#'  # mark visited<br>        for dr, dc in [(0,1),(0,-1),(1,0),(-1,0)]:<br>            dfs(r+dr, c+dc, node.children[char], path+char)<br>        board[r][c] = char  # restore<br>    for r in range(rows):<br>        for c in range(cols):<br>            dfs(r, c, trie.root, "")<br>    return list(result)</code></pre><br><b>Without trie:</b> check each word independently = O(words × m×n × 4^L). <b>With trie:</b> search all words simultaneously by following trie branches during DFS.	tries::patterns
How do you implement autocomplete / search suggestions with a trie?	Store frequency at word-end nodes. On prefix input, find all matching words and rank by frequency:<br><pre><code>class AutoComplete:<br>    def __init__(self):<br>        self.root = TrieNode()  # add 'freq' field to TrieNode<br><br>    def insert(self, word, freq=1):<br>        node = self.root<br>        for char in word:<br>            if char not in node.children:<br>                node.children[char] = TrieNode()<br>            node = node.children[char]<br>        node.is_end = True<br>        node.freq = freq<br><br>    def suggest(self, prefix, max_results=5):<br>        node = self.root<br>        for char in prefix:<br>            if char not in node.children: return []<br>            node = node.children[char]<br>        results = []<br>        def dfs(node, word):<br>            if node.is_end:<br>                results.append((word, node.freq))<br>            for ch, child in node.children.items():<br>                dfs(child, word + ch)<br>        dfs(node, prefix)<br>        results.sort(key=lambda x: -x[1])<br>        return [w for w, f in results[:max_results]]</code></pre><br>This is the real-world autocomplete pattern — Google search, IDE code completion, and T9 predictive text all use trie variants.	tries::patterns
How does a T9 dictionary use a trie?	T9 maps digits to letter groups (2=ABC, 3=DEF...). Build a trie keyed by <b>digits</b> instead of letters, storing actual words at each node:<br><pre><code>class T9Trie:<br>    def __init__(self):<br>        self.root = TrieNode()<br>        self.letter_to_digit = {}<br>        for digit, letters in {'2':'abc','3':'def','4':'ghi',<br>            '5':'jkl','6':'mno','7':'pqrs','8':'tuv','9':'wxyz'}.items():<br>            for ch in letters:<br>                self.letter_to_digit[ch] = digit<br><br>    def insert(self, word):<br>        node = self.root<br>        for ch in word.lower():<br>            digit = self.letter_to_digit.get(ch, '')<br>            if digit not in node.children:<br>                node.children[digit] = TrieNode()<br>            node = node.children[digit]<br>            if word not in getattr(node, 'words', []):<br>                node.words = getattr(node, 'words', []) + [word]<br>        node.is_end = True</code></pre><br>Input "4663" navigates the digit trie to find "gone", "good", "home", etc. The trie shares structure among words with the same digit pattern.	tries::patterns
How does the "Replace Words" problem use a trie?	Given a dictionary of roots and a sentence, replace each word with its shortest root. Build a trie from roots, then check each word:<br><pre><code>def replaceWords(dictionary, sentence):<br>    trie = Trie()<br>    for root in dictionary:<br>        trie.insert(root)<br>    def findRoot(word):<br>        node = trie.root<br>        for i, char in enumerate(word):<br>            if char not in node.children:<br>                return word  # no root found<br>            node = node.children[char]<br>            if node.is_end:<br>                return word[:i+1]  # shortest root<br>        return word<br>    return ' '.join(findRoot(w) for w in sentence.split())</code></pre><br><b>Time: O(n × k)</b> where n = words in sentence, k = average word length. The trie naturally finds the <b>shortest</b> matching prefix — just stop at the first <code>is_end</code> node encountered during traversal.	tries::patterns
How do you implement a trie that supports wildcard search (e.g., "b.d" matches "bad", "bed")?	When encountering a wildcard '.', branch into ALL children instead of one specific child:<br><pre><code>def search(self, word):<br>    def dfs(node, i):<br>        if i == len(word):<br>            return node.is_end<br>        char = word[i]<br>        if char == '.':<br>            for child in node.children.values():<br>                if dfs(child, i + 1):<br>                    return True<br>            return False<br>        else:<br>            if char not in node.children:<br>                return False<br>            return dfs(node.children[char], i + 1)<br>    return dfs(self.root, 0)</code></pre><br><b>Time: O(26^m × k)</b> worst case where m = number of wildcards, but usually much faster due to pruning. This is the "Design Add and Search Words" LeetCode problem. Without wildcards, it degrades to normal O(k) trie search.	tries::patterns
What is a compressed trie (radix tree) and when is it useful?	A <b>compressed trie</b> merges chains of single-child nodes into one edge with a multi-character label:<br><pre><code>Standard trie:        Compressed trie:<br>  root                  root<br>  |                     / \<br>  t                  "test"  "toast"<br>  |                     |<br>  e   o              "ing"<br>  |   |<br>  s   a<br>  |   |<br>  t   s<br>  |   |<br> [i]  t<br>  |<br>  n<br>  |<br>  g</code></pre><br><b>Advantages:</b> Uses far less memory when many nodes have single children (common in long words with unique suffixes). Same O(k) time complexity for operations.<br><br><b>Used in:</b> IP routing tables (longest prefix match), file systems, and any scenario with long strings and sparse branching.	tries::fundamentals
What are the key differences between a trie, hash map, and BST for string storage?	<b>Trie:</b><br>• Search/insert: O(k) where k = word length<br>• Prefix operations: O(k) — native support<br>• Sorted iteration: yes (DFS gives alphabetical order)<br>• Space: high (pointer overhead per node)<br><br><b>Hash Map:</b><br>• Search/insert: O(k) average (hashing cost)<br>• Prefix operations: O(n × k) — must check all keys<br>• Sorted iteration: no<br>• Space: moderate<br><br><b>Balanced BST (e.g., TreeMap):</b><br>• Search/insert: O(k log n) — compare strings at each node<br>• Prefix operations: O(k log n + matches)<br>• Sorted iteration: yes<br>• Space: moderate<br><br><b>Bottom line:</b> Trie wins when prefix operations matter. Hash map wins for pure key lookup. BST wins when you need sorted order with range queries.	tries::fundamentals
What is a segment tree and when do you use it?	A <b>segment tree</b> is a binary tree for efficient <b>range queries</b> (sum, min, max, GCD) and <b>point/range updates</b> on arrays:<br><pre><code># Build: O(n), Query: O(log n), Update: O(log n)<br>class SegmentTree:<br>    def __init__(self, nums):<br>        self.n = len(nums)<br>        self.tree = [0] * (4 * self.n)<br>        self._build(nums, 0, 0, self.n - 1)<br><br>    def _build(self, nums, node, start, end):<br>        if start == end:<br>            self.tree[node] = nums[start]<br>            return<br>        mid = (start + end) // 2<br>        self._build(nums, 2*node+1, start, mid)<br>        self._build(nums, 2*node+2, mid+1, end)<br>        self.tree[node] = self.tree[2*node+1] + self.tree[2*node+2]</code></pre><br>Each node stores an aggregate for a range [l, r]. Leaves = individual elements. Parent = merge of children's ranges.<br><br><b>Use when:</b> You need both range queries AND updates. If only queries (no updates), use prefix sums instead. Common in competitive programming.	advanced_trees::segment_tree
What is a Fenwick Tree (Binary Indexed Tree) and how does it compare to a segment tree?	A <b>Fenwick tree</b> uses bit manipulation to maintain prefix sums with point updates in O(log n):<br><pre><code>class FenwickTree:<br>    def __init__(self, n):<br>        self.n = n<br>        self.tree = [0] * (n + 1)  # 1-indexed<br><br>    def update(self, i, delta):  # O(log n)<br>        i += 1  # 1-indexed<br>        while i <= self.n:<br>            self.tree[i] += delta<br>            i += i & (-i)  # add lowest set bit<br><br>    def query(self, i):  # prefix sum [0, i], O(log n)<br>        i += 1<br>        total = 0<br>        while i > 0:<br>            total += self.tree[i]<br>            i -= i & (-i)  # remove lowest set bit<br>        return total<br><br>    def range_query(self, l, r):  # sum [l, r]<br>        return self.query(r) - (self.query(l-1) if l > 0 else 0)</code></pre><br><b>vs Segment Tree:</b> Fenwick is simpler, lower constant factor, less memory (n vs 4n). But segment trees support range updates with lazy propagation and more complex operations (min/max). Use Fenwick for prefix sums with updates; segment tree for everything else.	advanced_trees::fenwick_tree
What is an AVL tree and how do rotations maintain balance?	An <b>AVL tree</b> is a self-balancing BST where left and right subtree heights differ by at most 1. When an insert/delete violates this, <b>rotations</b> restore balance:<br><br><b>4 rotation cases:</b><br>• <b>Left-Left (LL):</b> Right rotate parent<br>• <b>Right-Right (RR):</b> Left rotate parent<br>• <b>Left-Right (LR):</b> Left rotate child, then right rotate parent<br>• <b>Right-Left (RL):</b> Right rotate child, then left rotate parent<br><br>After rotation, the subtree height decreases by 1, restoring the balance factor to {-1, 0, 1}.<br><br><b>All operations: O(log n)</b> guaranteed. More strictly balanced than Red-Black trees → faster lookups, but more rotations on insert/delete. Rarely implemented in interviews, but know the concept for discussing BST worst-case fixes.	advanced_trees::avl
What is a Red-Black tree and how does it differ from AVL?	A <b>Red-Black tree</b> is a self-balancing BST with colored nodes following these rules:<br>1. Every node is red or black<br>2. Root is black<br>3. No two consecutive red nodes (red parent → black children)<br>4. Every root-to-null path has the same number of black nodes<br><br><b>vs AVL:</b><br>• Red-Black: height ≤ 2 log(n+1). AVL: height ≤ 1.44 log(n).<br>• Red-Black: fewer rotations on insert/delete → better for write-heavy workloads<br>• AVL: more strictly balanced → faster lookups<br><br><b>Real-world usage:</b> Java's TreeMap/TreeSet, C++ std::map, Linux kernel scheduler. Python's <code>sortedcontainers</code> uses a different approach (sorted list of sublists). You won't implement these in interviews, but knowing they guarantee O(log n) is important.	advanced_trees::red_black
What is a B-Tree and why is it used in databases?	A <b>B-Tree</b> is a self-balancing tree where nodes can have <b>many children</b> (order m = hundreds or thousands), keeping the tree very shallow:<br><br>• All leaves at the same depth<br>• Each node stores up to m-1 keys<br>• Internal nodes have between ⌈m/2⌉ and m children<br>• All operations: <b>O(log n)</b><br><br><b>Why databases use B-Trees:</b> Disk reads are slow. A node = one disk page (4KB). With m=1000, a tree with 1 billion keys has height ≈ 3 → only 3 disk reads to find any key!<br><br><b>B+ Tree:</b> Variant where all data lives in leaves and leaves are linked for efficient range scans. Most SQL databases (PostgreSQL, MySQL InnoDB) use B+ Trees for indexes.	advanced_trees::b_tree
How does Union-Find (Disjoint Set Union) use tree structure?	Union-Find tracks connected components using trees where each element points to a parent. The root identifies the component:<br><pre><code>class UnionFind:<br>    def __init__(self, n):<br>        self.parent = list(range(n))<br>        self.rank = [0] * n<br><br>    def find(self, x):  # path compression<br>        if self.parent[x] != x:<br>            self.parent[x] = self.find(self.parent[x])<br>        return self.parent[x]<br><br>    def union(self, x, y):  # union by rank<br>        px, py = self.find(x), self.find(y)<br>        if px == py: return False<br>        if self.rank[px] < self.rank[py]:<br>            px, py = py, px<br>        self.parent[py] = px<br>        if self.rank[px] == self.rank[py]:<br>            self.rank[px] += 1<br>        return True</code></pre><br><b>Find/Union: O(α(n)) ≈ O(1)</b> amortized with path compression + union by rank. α is the inverse Ackermann function — effectively constant.<br><br><b>Use for:</b> Graph connectivity, Kruskal's MST, cycle detection, connected components.	advanced_trees::union_find
What is a suffix tree/suffix array and when would you use it?	<b>Suffix tree:</b> A compressed trie of ALL suffixes of a string. Enables O(k) substring search in a text of any length.<br><br>For "banana": suffixes are "banana", "anana", "nana", "ana", "na", "a" — all inserted into a compressed trie.<br><br><b>Suffix array:</b> A sorted array of all suffix starting indices. Uses less space than a suffix tree.<br><br><b>Use cases:</b><br>• Finding if a pattern exists in a text: O(k) with suffix tree<br>• Longest repeated substring<br>• Longest common substring of two strings<br>• DNA sequence analysis<br><br><b>Rarely asked in interviews</b> due to implementation complexity, but mentioning them shows advanced knowledge. Python's <code>str.find()</code> uses simpler algorithms for most cases.	advanced_trees::suffix_structures
What is a Skip List and how does it achieve O(log n) search?	A <b>skip list</b> is a probabilistic data structure: a linked list with express lanes. Multiple levels of linked lists, where each higher level skips more elements:<br><pre><code>Level 3:  1 ————————————→ 9<br>Level 2:  1 ——→ 4 ——————→ 9<br>Level 1:  1 → 3 → 4 → 6 → 9<br>Level 0:  1 → 2 → 3 → 4 → 5 → 6 → 7 → 8 → 9</code></pre><br><b>Search:</b> Start at top level, move right until overshoot, drop down one level. Repeat. Average O(log n).<br><br><b>Insert:</b> Insert at level 0, then "coin flip" to decide if promoted to higher levels.<br><br><b>vs Balanced BST:</b> Simpler to implement, same expected complexity, but not guaranteed worst-case. Used in Redis sorted sets and LevelDB/RocksDB.	advanced_trees::skip_list
When would you use each type of advanced tree structure?	<b>Decision guide:</b><br><br>• Need prefix matching / autocomplete → <b>Trie</b><br>• Need range queries + updates on array → <b>Segment Tree</b><br>• Need prefix sums + point updates → <b>Fenwick Tree</b> (simpler)<br>• Need guaranteed O(log n) BST operations → <b>AVL or Red-Black Tree</b><br>• Need disk-friendly indexing → <b>B-Tree / B+ Tree</b><br>• Need connected components / union → <b>Union-Find</b><br>• Need substring search in large text → <b>Suffix Tree/Array</b><br>• Need sorted set with O(log n) ops, simple implementation → <b>Skip List</b><br><br><b>Interview frequency:</b> Trie (very common), Union-Find (common in graph problems), Segment/Fenwick (rare, more competitive programming), others (conceptual only).	advanced_trees::overview
